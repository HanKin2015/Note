[TOC]

# 机器学习实战（20170820）

# 第一部分 分类
## 第1章 机器学习基础

### 1.1 前言
+ 监督学习一般使用两种类型的目标变量：标称型和数值型。标称型目标变量的结果只在有限目标集中取值，如真与假、动物分类集合{爬行类、鱼类、哺乳类、两栖类、植物、真菌 };数值型目标变量则可以从无限的数值集合中取值，如 0.100、42.001、1000.743等。数值型目标变量主要用于回归分析。
+ 简单地说，机器学习就是把无序的数据转换成有用的信息。
+ 机器学习的主要任务，并给出一个表格，帮助读者将机器学习算法转化为可实际运作的应用程序。如何解决分类问题，它的主要任务是将实例黎据划分到合适的分类中。
+ 机器学习的另一项任务是回归，它主要用于预测数值型数据。大多数人可能都见过回归的
例子— 数据拟合曲鉍：通过给定数据点的最优拟合曲线。分类和回归属于监督学习，之所以称
之为监督学习，是因为这类算法必须知道预测什么，即目标变量的分类信息。
+ 与监督学习相对应的是无监督学习，此时数据没有类别信息，也不会给定目标值。在无监督学习中，将数据集合分成由类似的对象组成的多个类的过程被称为聚类；将寻找描述数据统计值的过程称之为密度估计。此外，无监督学习还可以减少数据特征的维度，以便我们可以使用二维或三维图形更加直观地展示数据信息。
---
	表1 - 2 用于执行分类、回归、聚类和密度估计的机器学习算法
	监督学习的用途
	k-近邻算法   	 线性回归
	朴素贝叶斯算法   局部加权线性回归
	支持向量机    	 Ridge回归
	决策树     		 Lasso最小回归系数估计
	无监督学习的用途
	K-均值   		 最大期望算法
	DBSCAN   		 Parzen窗设计
---
### 1.2 开发机器学习应用程序的步骤
(1 )收集数据。如 ：制作网络爬虫从网站上抽取数据、从RSS反馈或者API中得到信息、设备发送过来的实测数据（风速、血糖等)。
(2 )准备输入数据。得到数据之后，还必须确保数据格式符合要求，本书采用的格式是Python语言的List。使用这种标准数据格式可以融合算法和数据源，方便匹配操作。
(3)分析输入数据。最简单的方法是用文本编辑器打开数据文件，査看得到的数据是否为空值。此外，还可以进一步浏览数据 ，分析是否可以识别出模式；数据中是否存在明显的异常值，如某些数据点与数据集中的其他值存在明显的差异。通过一维、二维或三维图形展示数据也是不错的方法，然而大多数时候我们得到数据的特征值都不会低于三个，无法一次图形化展示所有特征。这一步的主要作用是确保数据集中没有垃圾数据。
(4)训练算法。机器学习算法从这一步才真正开始学习。根据算法的不同，第4步和第5步是机器学习算法的核心。我们将前两步得到的格式化数据输入到算法，从中抽取知识或信息。这里得到的知识需要存储为计算机可以处理的格式，方便后续步骤使用。如果使用无监督学习算法，由于不存在目标变量值，故而也不需要训练算法，所有与算法相关的内容都集中在第5步。
(5)测试算法 。这一步将实际使用第4步机器学习得到的知识信息。为了评估算法，必须测试算法工作的效果。对于监督学习，必须已知用于评估算法的目标变量值；对于无监督学习，也必须用其他的评测手段来检验算法的成功率。
( 6 )使用算法。将机器学习算法转换为应用程序，执行实际任务，以检验上述步骤是否可以在实际环境中正常工作。
---
之所以选择Python,是因为它具有其他编程语言不具备的优势，如易于理解、丰富的函数库（尤其是矩阵操作)、活跃的开发者社区等。
基于以下三个原因，我们选择Python作为实现机器学习算法的编程语言：(1)Python语法清晰；(2)易于操作纯文本文件；(3)使用广泛，存在大量的开发文档。
---
	NumPy矩阵与数组的区别
	NumPy函数库中存在两种不同的数据类型（矩阵matrix和数组array），都可以用于处理行列表示的数字元素。虽然它们看起来很相
	似，但是在这两个数据类型上执行相同的数学运算可能得到不同的结果，其中NumPy函数库中的matrix与MATLAB 中matrices等价。
	调用mat()函数可以将数组转化为矩阵。
---
## 第2章 k-近邻算法
简单地说，k近邻算法采用测量不同特征值之间的距离方法进行分类。

优点：精度高、对异常值不敏感、无数据输入假定。
缺点：计算复杂度高、空间复杂度高。
适用数据范围：数值型和标称型。
---
k-近邻算法的一般流程：
(1)收集数据：可以使用任何方法。
(2)准备数据：距离计算所需要的数值，最好是结构化的数据格式。
(3)分析数据：可以使用任何方法。
(4)训练算法：此步驟不适用于k-近邻算法。
(5)测试算法：计算错误率。
(6)使用算法：首先需要输入样本数据和结构化的输出结果，然后运行k-近邻算法判定输入数据分别属于哪个分类，最后应用对计算出的分类执行后续的处理。
---
欧式距离公式
Python strip() 方法用于移除字符串头尾指定的字符（默认为空格）回车字符
reload() 用于重新载入之前载入的模块。
---
### 实战：约会
1. 准备数据：将文本记录到转换NumPy的解析程序中（见代码）
2. 分析数据（数据可视化）：使用Matpllotlib创建散点图
3. 准备数据：归一化数值
4. 分类器针对约会网站的测试代码
5. 约会网站预测函数

在处理这种不同取值范围的特征值时，我们通常采用的方法是将数值归一化，如将取值范围处理为0到1或者-1到1之间。下面的公式可以将任意取值范围的特征值转化为0到1区间内的值：
newValue = {oldValue - min) / (max-min)
其中min和max分别是数据集中的最小特征值和最大特征值。虽然改变数值取值范围增加了分类器的复杂度，但为了得到准确结果，我们必须这样做。
tile函数：倒着看，tile(a,[2,3])  #第一维重复3遍，然后重复两遍
axis参数：0表示列，1表示行。
axes：坐标轴
raw_input():允许用户输入文本行命令并返回用户所输入的命令，但3.X版本使用了input
python中换行继续用反斜杠\，所以md文件中用两个转义字符。

### 实战：手写识别系统
灰度处理
需要识别的数字已经使用图形处理软件，处理成具有相同的色彩和大小: 宽髙是32像素x32像素的黑白图像。尽管采用文本格式存储图像不能有效地利用内存空间，但是为了方便理解，我们还是将图像转换为文本格式。
---
(1)收集数据：提供文本文件。
(2)准备数据：编写函数classify, 将图像格式转换为分类器使用的list格式。
(3)分析数据：在python命令提示符中检查数据，确保它符合要求。
(4)训练算法：此步驟不适用于k-近邻算法。
(5)测试算法：编写函数使用提供的部分数据集作为测试样本，测试样本与非测试样本的区别在于测试样本是已经完成分类的数据，如果预测分类与实际类别不同，则标记为一个错误。
(6)使用算法：本例没有完成此步驟，若你感兴趣可以构建完整的应用程序，从图像中提取数字，并完成数字识别，美国的邮件分拣系统就是一个实际运行的类似系统。

### 小结
k决策树就是k-近邻算法的优化版，可以节省大量的计算开销。
k-近邻算法是分类数据最简单最有效的算法，本章通过两个例子讲述了如何使用k-近邻算法构造分类器。k近邻算法是基于实例的学习，使用算法时我们必须有接近实际数据的训练样本数据。k-近邻算法必须保存全部数据集，如果训练数据集的很大，必须使用大量的存储空间。此外,由于必须对数据集中的每个数据计算距离值，实际使用时可能非常耗时。k近邻算法的另一个缺陷是它无法给出任何数据的基础结构信息，因此我们也无法知晓平均实例样本和典型实例样本具有什么特征。

## 第3章 决策树
二十个问题游戏
优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。
缺点：可能会产生过度匹配问题。
适用数据类型：数值型和标称型。

在构造决策树时，我们需要解决的第一个问题就是，当前数据集上哪个特征在划分数据分类时起决定性作用。为了找到决定性的特征，划分出最好的结果，我们必须评估每个特征。
量化
二分法
递归，离散化，标称型和数值型
标称型：标称型目标变量的结果只在有限目标集中取值，如真与假(标称型目标变量主要用于分类)

数值型：数值型目标变量则可以从无限的数值集合中取值，如0.100，42.001等 (数值型目标变量主要用于回归分析)

决策树的一般流程
(1)收集数据：可以使用任何方法。
(2)准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。
(3)分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。
(4)训练算法：构造树的数据结构。
(5)测试算法：使用经验树计算错误率。
(6)使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。

在划分数据集之前之后信息发生的变化称为信息增益。
集合信息的度量方式称为香农熵或者简称为熵，这个名字来源于信息论之父克劳德•香农。
熵定义为信息的期望值，在明晰这个概念之前，我们必须知道信息的定义。如果待分类的事务可能划分在多个分类之中，则符号$x_i$的信息定义为
$$l(x_i) = -log_2p(x_i)$$
其中$p(x_i)$是选择该分类的概率。
为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值，通过下面的公式得到：
$$H =- \sum_{i=1}^n p(x_i)log_2p(x_i)$$
其中n是分类的数目。

reload()函数
1. 创建数据集
2. 计算给定的数据集的香农熵。
3. 

熵越高，则混合的数据也越多，我们可以在数据集中添加更多的分类，观察熵是如何变化的（值变大）。
另一个度量集合无序程度的方法是基尼不纯度(Gini impurity) , 简单地说就是从一个数据集中随机选取子项，度量其被错误分类到其他分组里的概率。
















