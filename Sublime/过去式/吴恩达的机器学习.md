<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
[TOC]
# 机器学习

- 准备机器学习和Linux一起开始学习 20170804
- 大牛机器学习网站http://www.cnblogs.com/tornadomeet/
###### 视频学习网站
Tom Mitchell 和 Andrew Ng 的课都很适合入门
- [openclassroom]()
- [couresa](https://www.coursera.org/learn/machine-learning/home/welcome)

--------------------------------------------
machine learning
review 复习，回顾，讲评
lecture 讲义，讲课，演讲
anti-spam 反垃圾邮件
building recommender systems 建立推荐系统

the term supervised learning 术语监督学习

regression回归


--------------------------------------------
# 监督学习
- 分类问题（离散）和回归问题（线性）
回归一词指的是我们根据之前的数据预测出一个准确的输出值。

##### 20170805
监督学习：根据数据集对每一个例子给出一个“正确”的答案。
Training set
m = Number of training examples
x^i = input
y^i = output
h = hypothesis假设

### 20170806
- linear regression线性回归
parameter 参数
- 线性回归函数:\\(h_\theta (x)=\theta _0+\theta _1x\\)
- 整体目标函数(最小二乘法)：\\(\mathop{minimize}\limits\_{\theta\_0 \theta\_1} = (h\_\theta (x) - y^i)^2\\)
- 代价函数：\\(\mathop{minimize}\limits_{\theta\_0 \theta\_1} = \frac{1}{2m} sum_{i=1}^m (h\_\theta (x) - y^i)^2\\)
- 平方误差代价函数:\\(\tau (\theta\_0,\theta\_1) = \frac{1}{2m} sum\_{i=1}^m (h\_\theta (x) - y^i)^2\\)
----------------------------------------------------------
- 将两个公式进行整合，失败！
- \\(43 = 43\\) \\(23 = 23\\)
- \\(\tau （\theta _0,\theta_1) = \\)
- \\(\frac{1}{2m} sum_{i=1}^m (h_\theta (x) - y^i)^2\\)
- 但是我找到了另一个办法，先使用代码生成，如果显示失败，再使用另一个网站生成图片，再md文件上添加图片(右键图片地址)即可。
- 网址1：http://zh.numberempire.com/texequationeditor/equationeditor.php
- 网址2：http://www.forkosh.com/mathtextutorial.html
----------------------------------------------------------
###### 总结
- Hypothesis:\\(h_\theta (x) = \theta _0 + \theta _1x\\)
- Parameters:\\(\theta _0,\theta _1\\)
- Cost Function:\\(\tau (\theta\_0,\theta\_1) = \frac{1}{2m} sum\_{i=1}^m (h\_\theta (x^i) - y^i)^2\\)
- Goal:\\(\mathop{minimize}\limits_{\theta _0 \theta _1} \tau (\theta _0,\theta_1)\\)
- 去不同的参数值，然后计算平方误差值，平方误差最小的代表拟合的最佳。
- contour plot或contour figure

##### 断网了
### 10、2-5-Gradient Descent algorithm梯度下降
outline
:= 表示赋值
= 表示判断是否相等
$$\theta \_j := \theta \_j - \alpha \frac{\vartheta}{\vartheta \theta \_j} J(\theta \_0,\theta \_1)  (for\ j = 0\ and\ j = 1)$$
$$\alpha表示learning\ rate$$
同时更新是梯度下降算法注意的。
偏导数和导数
converge汇集，汇合
local optima局部最优
“Batch” Gradient Descent批处理梯度下降

- 第一个机器学习算法
repeat until convergence {
\\(\theta\_0:=\theta\_0-\alpha\frac{1}{m}\sum\_{i=1}^m(h\_\theta(x^i)-y^i)\\)
	\\(\theta\_1:=\theta\_1-\alpha\frac{1}{m}\sum\_{i=1}^m(h\_\theta(x^i)-y^i)*x^i\\)
}

### 13、3-1-Matrices and Vectors
1-indexed vs 0-indexed:默认是1-indexed
ABCX表示矩阵
abxy表示向量
octave语言
加减乘除addition  subtraction multiplication division
identity Matrix单位矩阵:I*A = A*I = A
|-   -|
|10000|
|01000|
|00100|
|00010|
|00001|
|-   -|
denoted I表示

Matrix inverse：只有方阵有逆矩阵
0没有倒数。同样0矩阵也没有逆矩阵
\\(AA^-1 = A^-1A = I\\)
没有逆矩阵的矩阵叫"singular" or "degenerate"奇异矩阵或者退化矩阵

上周工作内容
1.看书《Managing and Mining Graph Data》
2.了解数据挖掘和图数据挖掘一些领域知识

下周计划
继续理解数据挖掘和图数据挖掘的一些领域知识

## 20170808
Matrix Transpose转置矩阵

### Multiple Features
n: number of features
x^i:
x_j^i:
m: number of samples

新的假设方程：\\(h_\theta (x) = \theta _0 + \theta _1x\_1 + \theta _2x\_2 + ... + \theta _nx\_n\\)
for convenience of notationg, define x_0 = 1.
向量空间用空心R表示。
向量一般是竖着（列向量）表示，所以它的转置矩阵是行向量。
即：
\\(h\_\theta (x) = \theta \_0 + \theta _1x\_1 + \theta _2x\_2 + ... + \theta _nx\_n\\)
\\(h\_\theta (x) = \theta^T*X\\)
向量内积
Multivariate linear regression多元回归方程
代价函数怎么来的？？？？？

Linear Algebra线性代数
 3-dimensional vector

### Practice
- feature scaling特征缩放---------特征除以最大值
get every feature into approximately a -1<=x_i<=1 range.
[-3,3]或者[-1/3,1/3]
Mean normalization 均值归一化
\\(x = \frac{x\_i - u\_i}{s\_i}\\)
u_i---avg value of x_i in training set
s_i---range(max - min)

iteration迭代
automatic
convergence收敛
Goal:35
特征归一化
polynomial regression多项式回归

## Normal equation标准方程法
design matrix
\\(\theta = (X^TX)^{-1}X^Ty\\)
Octave:pinv(X'*X)*X'*y

## VS
Gradient Descent
- Need to choose a
- Needs many iterations
- works well even when n is large

Normal Equation(千级别及以内)
- No need to choose a
- Don't need to iterate
- Need to compute(X^TX)^-1
- Slow if n is very large

## What if X^T*X is non-invertible
伪逆函数pinv()   vs  inv()
- redundant features(linearly dependent)冗余特征（线性相关）
- too many features
	- Delete some features, or use regularization

e.g. exempli gratia =for example 举例来说，例如。随时可以插入
dimension尺寸

## Octave基本语法
+-*/
%  注释
== 等于 
~= 不等于
1 && 2    AND
1 && 0    OR
xor(1,0)  XOR
PS1('>> ');	  隐藏命令
a = 3;    %赋值，加分号可以抑制结果在屏幕上显示
a = pi;   %直接输入变量名a即可打印出结果，或者使用disp(a);
disp(sprintf('2 decimals: %0.2f', a))    %C语言风格输出，sprintf转换成字符串
format long 长格式控制
format short短格式控制
A = [1 2; 3 4; 5 6]  %矩阵
V = 1:0.1:2   %从1开始，以步长0.1增加到2，默认步长为1
C = 2*ones(2,3)  %设生成2行3列的全2矩阵
C = zeros(2,3)  %设生成2行3列的全0矩阵
W = rand(1,3)   %1行3列的随机值矩阵，数值在0~1之间
Gaussian random variable高斯随机变量
正态分布randn(1,3)
hist(w,50)    %直方图
eye(4)        %4*4的单位矩阵
help eye      %帮助文档

### 第26节将作业上传
特征缩放就是最大值
均值归一化
平均值为 (7921+5184+8836+4761)/4=6675.5
Max-Min=8836-4761=4075
(4761-6675.5)/4075=-0.46957
保留两位小数为-0.47 

#### 监督学习
监督学习是指：利用一组已知类别的样本调整分类器的参数，使其达到所要求性能的过程，也称为监督训练或有教师学习。
监督学习是从标记的训练数据来推断一个功能的机器学习任务。训练数据包括一套训练示例。在监督学习中，每个实例都是由一个输入对象（通常为矢量）和一个期望的输出值（也称为监督信号）组成。监督学习算法是分析该训练数据，并产生一个推断的功能，其可以用于映射出新的实例。一个最佳的方案将允许该算法来正确地决定那些看不见的实例的类标签。这就要求学习算法是在一种“合理”的方式从一种从训练数据到看不见的情况下形成。

通常可以考虑尝试学习率：
a = 0.01, 0.03, 0.1, 0.3, 1, 3, 10

迭代次数的减少，加快了正确答案的得出。正规方程对计算只与训练集的大小有关，而与值无关，不能阻止梯度下降局部最优(ps:正规方程没有局部最优)，第四个答案，除非可以减少特征变量，否则不能解决此问题所以选A

第四题Answer:第一个根据垃圾邮件发现垃圾邮件的子类型，没有给正确答案属于分类，第四个搜集1000篇文章，没有给文章的类型，以及应该怎么分类，故属于非监督学习。PS：界定监督学习与非监督学习，看给的数据是否有想要的正确答案即可选择BC。

### 将数据加载到octave中
size(A)  % 3  2
size(A,1) %返回行数
size(A,2) %返回列数
length(A) %输出最大维数3
pwd  %显示当前路径
cd 'C:desktop'  %修改路径
ls   %显示文件

load featuresX.dat = load('featureX.dat')  %后者将文件名以字符串
who命令显示当前octave存储的变量
导入后输入文件名即可显示文件内容
whos可以更详细查看变量
clear+变量名  %清理变量
v = priceY(1:10)  %将priceY数组前10个元素存储
save helo.mat v;  %将v内容存储到hello.mat文件中
clear  %清理所有变量
save hello.txt v -ascii  %sace as text(ASCII)
A(3,2)  %一个数组元素
A(3,:)  %数组第三行所有元素
A(:,2)  %数组第二列所有元素
A([1 3],:) %数组第一行和第三行所有元素
A(:,2) = [10; 11; 12] %替换第二列元素
A = [A, [100; 101; 102]]; %在A数组后面增加一列
A(:) %put all elments of A into single vector
C = [A B]  %连接A和B，中间可以加逗号
C = [A; B] %将B放到A下面合起来

## 20170809
A*B
A .* B  %点乘运算，使A中元素对应相乘
A .^ 2
1 ./ A
v = [1;2;3]
log(v)
exp(v)  以自然数e为底
abs(v)  取绝对值
-v    % -1*v
v + ones(length(v),1)  %给v加1   % = v + 1
A'   %转置矩阵
[val, ind] = max(v)   %val存储最大值，ind存储索引值
max(A)  %对矩阵每一列求最大值
v < 2  % 对矩阵中每一个元素进行判断，返回01值
find(v < 2)  %返回小于2的索引值
魔方阵或幻方 magic(3) 行列对角线加起来相等
[r, c] = find(A >= 7)  %r,c分别表示行列索引值
sum(v) %=所有元素求和
prod(v) %所有元素乘积
floor(v) 向上取整
ceil(v)  向下取整
rand(3)
max(rand(3), rand(3))
max(A, [], 1)  取每列最大值
max(A, [], 2)  取每行最大值
求矩阵中最大值max(max(A))或者A(:)-->max(A(:))
sum(A,1) or sum(A,2)
sum(sum(A .* eye(9))) %主对角线的元素和
sum(sum(A .* flipud(eye(9))))  %副对角线的元素和
flipup 向上翻转
flipud 向下翻转
pinv(A) 矩阵求逆

### 绘制图像
y = sin(2*pi*x);
plot(x,y);
hold on;
m = cos(2*pi*n);
plot(n,m,'r')   %将两个图像绘制在一起,r表示颜色
xlabel('time')
ylabel('value') %标记横纵坐标名称
legend('sin','cos') %传说，说明，图例
title('my plot')

cd 'C:\Users\ang';
print -dpng 'myPlot.png' %保存图像
close关闭窗口
figure(1);
plot(x,y);
figure(2);  %打开第一个窗口
plot(n,m);
subplot(1, 2, 1);  %把窗口分成1*2的格子，并且调用第一个格子
plot(x,y);   %绘制图像在第一个格子上
subplot(1, 2, 2);
plot(n,m);

axis([0.5 1 -1 1]) %给坐标添加范围x(0.5~1),y(-1~1)
clf; 清理图像
imagesc(A)   %矩阵可视化
colorbar
imagesc(A), colorbar, colormap gray;  %执行三条命令
空格没有意义

for i=1:10,
	v(i) = 2^i;
end;
v

i = 1;
while true,
	v(i) = 999;
	i = i + 1;
	if i == 6,
		break;
	end;
end;
v

disp('da');  输出语句
elseif

exit或quit退出octave
#### 如何定义和调用函数
文件：squareThisNumber.m
function y = squareThisNumber(x)
y = x ^ 2;

> squareThisNumber(5)
addpath('C:\Users\ang\Desktop') %Octave search path(advanced/optional)

函数可以返回两个值或多个
文件：squareThisNumber.m
function [y1, y2] = squareThisNumber(x)
y1 = x ^ 2;
y2 = x + x;
> [a, b] = squareThisNumber(5);

#### 实战
X = [1 1;1 2;1 3];
Y = [1;2;3];
theta = [0;1];
J = costFunctionJ(X,Y,theta); %函数已经定义好

### 787集完
matlab下标是从1开始
向量化
simultaneous update同时更新

### 最流行的学习算法
classification
0:"negative class"
1:"positive class"
spam垃圾邮件
阈值：临界值，界限
logistic regression：分类算法
want \\(0<=h\_\theta(x)<=1\\)
\\(g(z) = \frac{1}{1+e^(-z)}\\)  Sigmoid function   or   Logistic function
\\(z = \theta^T*x\\)
==>\\(h\_\theta(x) = \frac{1}{1+e^(-\theta^Tx)}\\) 

## 20170810
假设函数:\\(h\_\theta(x) = p(y = 1|x;\theta)\\)  y=1在x和theta的条件下的概率

### Decision Boundary决策边界（阈值）
column 列
row 行
注意：行向量和列向量需要注意一下顺序，相乘会得到一个方阵。
syntax语法
outline大纲，概要
Modify修改
直接输入文件名即可运行m文件，CTRL+C可以阻止octave的运行。
token象征，符记，代币
negative value负值
scatter plot散点图
爱情面包分别代指"家庭"和"事业".出自一个曾经很流行的,女人很喜欢问男人的问题:"你是要爱情还是要面包?"
如果是聪明的男人就应该答:"都要!"
visualization可视化

重点：octave下标全部是从1开始！！！
clear ;  清除变量
close all;  关闭窗口
clc    清理屏幕

Non-linear decision boundaries
Cost function:
- Linear regression  非凸函数，不一点会找到全局最小值
- Logistic regression convx光滑的凸函数，会找到全局最小值

Logistic regression Cost function:

$$
Cost(h\_\theta(x),y) =
\begin{cases}
-log(h\_\theta(x)),  & \text{if $y = 1$} \\\
-log(1 - h\_\theta(x)), & \text{if $y = 0$}
\end{cases}
$$

Note:y = 0 or 1 always
$$
Cost(h_\theta(x),y) = -ylog(h_\theta(x)) - (1-y)log(1 - h_\theta(x))
$$
 
代价函数：$J(\theta) = \frac{1}{m}\sum_{i=1}^mCost(h_\theta(x^i),y^i)$  
$J(\theta) = -\frac{1}{m}[\sum_{i=1}^my^ilog(h_\theta(x^i)) + (1-y^i)log(1 - h_\theta(x^i))]$  
极大释然估计：

### 38-6-6完
multiclass classification
binary classification

## 20170811
- underfitting  --  high bias高偏差
- just right
- overfitting  --  high variance高方差
overfitting：样本数量少，特征量多容易出现过拟合。

#### 避免过拟合
1、Reduce number of features.
2、Regularization正规化

$J(\theta) = \frac{1}{2m}[\sum_{i = 1}^m(h_\theta(x^i) - y^i)^2 + \lambda\sum_{j=1}^n\theta_j^2]$
$\lambda$是正规化参数，正规化目标函数

Non-invertibility (optional/advanced)  
Suppose m <= n,  
    (#examples) (#features)  

$\theta = (X^TX)^{-1}X^Ty$  

if $\lambda$ > 0,  
$\theta = (X^TX + \lambda\begin{bmatrix}\\
 0  &  &  & \\ 
 &  1  &  & \\ 
 &  &  .  & \\ 
 &  &  &  1 
\end{bmatrix})^{-1}X^Ty$

### regularized logistic regression正则化避免过拟合现象  
在后面添加防止theta1后面的参数值过大。

### 43未完
## 20170812(目标55，争取啊！！)
octave中下标是从1开始的，但一般数学中是从0开始的。  
Adcanced optimization  
function [jVal, gradient] = costFunction(theta)  
jVal = [code to compute J(theta)];  
$J(\theta) = [-\frac{1}{m}\sum_{i = 1}^my^ilog(h_\theta(x^i) + (1-y^i)log(1 - h_\theta(x^i))] + \frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2]$  

gradient(n+1) = [code to compute$$];  

$J(\theta) = (\frac{1}{m}\sum_{i = 1}^m(h_\theta(x^i) - y^ix_n^i) + \frac{\lambda}{m}\theta_n]$  
线性回归、逻辑回归、高级优化算法、正则化技术  

### Neural Networks： representation表示，图像
Non-linear hypotheses  
pixel像素  
制造能模拟mimic大脑的机器  
Auditory Cortex听觉皮层  
input wires--dendrite     
output wires--Axon  
x0=1 bias unit/neuron
neuron神经元 cell 细胞  
input layer  
hidden layer  
output layer  
$a_i^j = "activation" of unit i in layer j$  
$\theta^j = matrix of weights controlling function mapping from layer j to layer j+1$

if netwoek S_j units in layer j, s_j + 1units in layer j+1, then theta^j will be of dimension s_(i+1)*(s_j+1).  

---
Forward propagation:Vectorized implementation  
$$
a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3)\\     
a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3)\\ 
a_2^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3)\\  
h_\Theta(x) = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(1)}a_1^{(2)} + \Theta_{12}^{(1)}a_2^{(2)} + \Theta_{13}^{(1)}a_3^{(2)})  
$$  
$$
x = \begin{bmatrix}\\
 x_0\\ 
 x_1\\ 
 x_2\\ 
 x_3 
    \end{bmatrix}
z^{(2)} = \begin{bmatrix}\\
 z_1^{(2)}\\ 
 z_2^{(2)}\\ 
 z_3^{(2)}
    \end{bmatrix}\\
z^{(2)} = \Theta^{(1)}a^{(1)}\\
a^{(2)} = g(z^{(2)})\\
Add\quad a_0^{(2)} = 1.\\
z^{(3)} = \Theta^{(2)}a^{(2)}\\
h_\Theta(x) = a^{(3)} = g(z^{(3)})
$$
---
Other network architectures  
手写数字识别--单一输出
图片分类--Multiple output units：One-vs-all
---
Neural Network(Classification)  
${(x^{(1)}, y^{(1)}),(x^{(2)}, y^{(2)}),..., (x^{(m)}, y^{(m)})}$  
L = total no. of layers in network  
s_l = no. of units (not counting bias unit) in layer l    

+ Binary classification  
y = 0 or 1  
1 output unit  
K = 1
S_L = 1  
一维向量

- Multi-class classification(K classes)
K output units  
K >= 3    
S_L = K  
K维向量
  
51未完

## 20170813(早上购物了，下午要努力啊，目标60)
Neural network：  
$J(\Theta)=-\frac{1}{m}[\sum_{i=1}^m\sum_{k=1}^Ky_k^{(i)}log(h_\Theta(x^{(i)}))_k + (1-y_k^{(i)}log(1 - h_\Theta(x^{(i)}))_k)] + \frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_l+1}(\Theta_{ji}^{(l)})^2]$   

forward propagation正向传播
$$
a^{(1)} = x\\
z^{(2)} = \Theta^{(1)}a^{(1)}\\
a^{(2)} = g(z^{(2)}) \quad(add\ a_0^{(2)})\\
z^{(3)} = \Theta^{(2)}a^{(2)}\\
a^{(3)} = g(z^{(3)}) \quad(add\ a_0^{(3)})\\
z^{(4)} = \Theta^{(3)}a^{(3)}\\
a^{(4)} = h_\Theta(x) = g(z^{(4)})
$$
Back propagation反向传播  
![](http://i1.bvimg.com/605023/ab59e70184f61c04.png)

### 55未完，逛超市。。
unroll摊开  
roll卷，滚动

    Theta1=ones(10,11)
    Theta2=2*ones(10,11)
    Theta3=3*ones(1,11)
    thetaVec=[Theta1(:);Theta2(:);Theta3(:)]
    size(thetaVec)
    reshape(thetaVec(1:110),10,11)
    reshape(thetaVec(111:220),10,11)
    reshape(thetaVec(221:231),1,11)

### Putting It Together
合理的默认规则：一个隐藏层，如果大于一个，每层常常会拥有相同的隐藏单元，隐藏层越多越好。

1. Try $\lambda=0$
2. Try $\lambda=0.01$
3. Try $\lambda=0.02$
4. Try $\lambda=0.04$
5. Try $\lambda=0.08$
...
12. Try $\lambda=10$
69完








