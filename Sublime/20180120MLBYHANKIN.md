---
layout: '[default_layout]'   
title: 励志算法工程师           
date: 2018-01-20 10:53:41  
updated: 
permalink: 
render_drafts: true
copyright: true
password: 
comments: true
toc: true                  
tags:                        
- ML
- Python

categories:                  
- ML

---
# 熟练掌握Python
- NumPy & SciPy高级的数学运算和科学运算
- Matplotlib绘图工具包
- Scikit-learn机器学习模型
- Pandas数据处理和分析
<!--more-->

# 编译环境
- Anaconda平台事半功倍
- IPython基于浏览器的解释器环境
- 最好在Linux环境下，毕竟看上去NB

# 1、k-近邻算法(kNN，k-NearestNeighbor)
简单地说，k近邻算法采用测量不同特征值之间的距离方法进行分类。
_行我素，_犬升天，_蜀山水，_雪纷飞。

不同的k取值可能得到不同的分类，而在应用中，k值一般取一个比较小的值，通常采用交叉验证法来选取最优的k值。通常k是不大于20的整数。

## 准备数据：归一化数值
离散化数据

# 2、决策树
1970年代，一个叫昆兰的大牛找到了用信息论中的熵来度量决策树的决策选择过程，方法一出，它的简洁和高效就引起了轰动，昆兰把这个算法叫做ID3。

熵是混乱程度的度量。

例如我们讨论太阳从哪升起。本来就只有一个结果，我们早就知道，那么无论谁传递任何信息都是没有信息量的。当可能结果数量比较大时，我们得到的新信息才有潜力拥有大信息量。

信息论基础里面：概率越大=可能性越大=信息量越少=不确定性越小=熵H（X）越小=自信息I（x）越小

互信息等价于信息增益。

并且在θ的所有取值上，使这个函数最大化。这个使可能性最大的值即被称为θ的最大似然估计。

“似然”是对likelihood 的一种较为贴近文言文的翻译，“似然”用现代的中文来说即“可能性”。故而，若称之为“最大可能性估计”则更加通俗易懂。

## ID3算法和C4.5算法
信息熵
信息增益-------------ID3算法
信息增益比(相对于条件的参数的信息熵的比值)----------C4.5算法

# 3、基于概率论的分类方法：朴素贝叶斯
选择高概率的类别（核心思想）
[](https://mp.weixin.qq.com/s/UXvSe_FYcS_s5HicMV6SUA)
另外对于某些属性缺失的样本带来条件概率为0的情况，通常采用拉普拉斯平滑处理进行修正。

之所以称之为“朴素”（naive），是假设样本x的各个属性之间是相互独立的，即各个属性对于预测结果的影响是相互独立的。Too young too simple，sometimes naive。

# 4、Logistic回归
海维塞德阶跃函数   单位阶跃函数   Sigmoid函数
梯度上升算法求函数的最大值，梯度下降算法用来求函数最小值

在每个特征上都乘以一个回归系数，然后把所有的结果值相加，将这个总和代入Sigmoid函数，得到0-1之间的数值。

密度函数就是对分布函数进行求导运算。


