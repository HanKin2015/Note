#### 模型变量-Variable
m 代表训练集中实例的数量
x 代表特征/输入变量
y 代表目标变量/输出变量
(x,y) 代表训练集中的实例
(x(i),y(i) ) 代表第 i 个观察实例
h 代表学习算法的解决方案或函数也称为假设（hypothesis）

#### 假设函数-hypothesis
h 代表 hypothesis(假设) ，h 表示一个函数，输入是房屋尺寸大小，就像你朋友想出售的房屋，因此 h 根据输入的 x 值来得出 y 值，y 值对应房子的价格 因此，h 是一个从x 到 y 的函数映射。

一种可能的表达方式为：$h_\theta=\theta_0+\theta_1x$， 因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题。

#### 代价函数-Cost Function
我们选择的参数决定了我们得到的直线相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距就是建模误差（modeling error）。

我们的目标便是选择出可以使得建模误差的平方和能够最小的模型参数。 即使得代价函数$J(\theta_0,\theta_1)$最小。

代价函数也被称作平方误差函数，有时也被称为平方误差代价函数。我们之所以要求出误差的平方和，是因为误差平方代价函数，对于大多数问题，特别是回归问题，都是一个合理的选择。还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是解决回归问题最常用的手段了。
在后续课程中，我们还会谈论其他的代价函数，但我们刚刚讲的选择是对于大多数线性回归问题非常合理的。

#### 梯度下降法-Gradient Descent
梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数$J(\theta_0,\theta_1)$的最小值。

梯度下降背后的思想是：开始时我们随机选择一个参数的组合（θ0,θ1,...,θn），计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到到一个局部最小值（local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（global minimum），选择不同的初始参数组合，可能会找到不同的局部最小值。

梯度下降算法公式：$\theta_j=\theta_j-\alpha\frac{\vartheta J(\theta_0,\theta_1,...)}{\vartheta\theta_j}$

如果你之前学过线性代数，有些同学之前可能已经学过高等线性代数，你应该知道有一种计算代价函数 J 最小值的数值解法，不需要梯度下降这种迭代算法。在后面的课程中，我们也会谈到这个方法，它可以在不需要多步梯度下降的情况下，也能解出代价函数 J 的最小值，这是另一种称为正规方程(normal equations)的方法。实际上在数据量较大的情况下，梯度下降法比正规方程要更适用一些。

#### 特征缩放-Feature Scaling
所有特征的尺度都尽量缩放到-1 到 1 之间。
$x_n=\frac{x_n-u_n}{s_n}$

#### 学习率
通常可以考虑尝试些学习率：α=0.01，0.03，0.1，0.3，1，3，10

#### 正规方程-Normal Equation






















