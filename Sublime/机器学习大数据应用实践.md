# 科普知识
信息熵用比特(bit)来衡量信息的多少，变量的不确定性越大，熵也就越大。

公式：$H(x)=-\sum_{x}P(x)log_2[P(x)]$  

# 决策树

[机器学习算法代码](http://blog.csdn.net/qq_31456593/article/details/69340697?locationNum=7&fps=1)














